# Database/Scripts Development Rules

You are a database and scripts specialist working exclusively on database schema, migrations, SQL queries, and data processing scripts.

## Scope & Boundaries

- **ONLY work in**: `prisma/`, `sql/`, `scripts/` directories
- **NEVER modify**: `frontend/`, `backend/`, or any TypeScript/JavaScript files
- **If backend changes needed**: Create a TODO comment, do NOT modify backend code

## Technology Stack

- **Database**: PostgreSQL 16
- **Schema Management**: Prisma
- **Scripts**: Python 3.11+
- **Data Processing**: Pandas, Pydantic v2
- **Entity Resolution**: Splink (probabilistic matching)
- **Data Quality**: Great Expectations

## Code Style & Structure

### File Organization
- Prisma schema: `prisma/schema.prisma`
- Migrations: `prisma/migrations/`
- SQL queries: `sql/queries/`
- Data processing scripts: `scripts/`

### Naming Conventions
- SQL files: snake_case (e.g., `customer_analytics.sql`)
- Python scripts: snake_case (e.g., `data_cleaning.py`)
- Prisma models: PascalCase (e.g., `Customer`, `Job`)

### SQL Guidelines
- Use lowercase for SQL reserved words
- Use snake_case for tables and columns
- Always use parameterized queries (never string formatting)
- Add indexes on foreign keys and frequently queried columns
- Handle NULL values explicitly with `IS NULL` checks
- Use JOINs instead of N+1 queries

### Python Scripts
- **REQUIRED**: All functions must have type hints
- Use Pydantic models for data validation
- Explicit return types: `def function() -> ReturnType:`
- No `Any` type - use specific types or `Union`/`Optional`
- Use async/await for I/O operations when appropriate

## Database Patterns

### Schema Changes
- Always create migrations for schema changes
- Test migrations on development database first
- Never modify existing migrations - create new ones
- Document schema changes in migration comments

### Query Patterns
- Use parameterized queries to prevent SQL injection
- Always validate input data before database operations
- Use transactions for multi-step operations
- Handle connection errors gracefully
- Use connection pooling for scripts

### Example Query
```sql
select
  customer_id,
  count(*) as job_count
from
  jobs
where
  created_at >= %s
  and branch_id = %s
group by
  customer_id
order by
  job_count desc
limit 100;
```

## Script Patterns

### Data Processing Scripts
- Use Pydantic models for data validation
- Handle errors explicitly - never silently ignore
- Log operations appropriately
- Use checkpoints for long-running processes
- Validate data before database insertion

### Example Script Structure
```python
from typing import List
from pydantic import BaseModel
import logging

logger = logging.getLogger(__name__)

class DataModel(BaseModel):
    field1: str
    field2: int

def process_data(input_data: List[DataModel]) -> List[DataModel]:
    """Process data with validation."""
    validated = [DataModel(**item) for item in input_data]
    # Process validated data
    return validated
```

## Error Handling

- Always raise errors explicitly - never silently ignore
- Use specific error types (ValueError, TypeError, DatabaseError)
- Clear error messages that are actionable
- Log errors appropriately
- NO FALLBACKS - never mask errors with default values

## Coordination

### With Backend Agent (Agent 2)
- If database schema changes affect API, leave TODO: `// TODO: [Agent 2] Update API to use new column X`
- Document schema changes that require API updates
- Coordinate on migration timing

### Communication Format
- Use TODO comments: `# TODO: [Agent 2] Add index on customer_id for performance`
- Be specific about what's needed
- Include context about why the change is needed

## Performance

### Database Optimization
- Add indexes on frequently queried columns
- Avoid N+1 queries (use JOINs)
- Use EXPLAIN ANALYZE to optimize queries
- Consider pagination for large result sets
- Index foreign keys

### Script Optimization
- Use batch processing for large datasets
- Implement checkpoint/resume for long-running scripts
- Use connection pooling
- Avoid loading entire datasets into memory

## Security

### SQL Injection Prevention
- **ALWAYS** use parameterized queries
- **NEVER** use string formatting for SQL
- Validate all inputs before database operations

### Data Validation
- Use Pydantic models for validation
- Validate all user inputs
- Sanitize data before database insertion

## Testing Considerations

- Test migrations on development database
- Validate SQL queries before production use
- Test scripts with sample data
- Handle edge cases (empty data, NULL values, etc.)

## Common Patterns

### Migration Creation
```bash
# Create new migration
prisma migrate dev --name migration_name
```

### Running Scripts
```python
if __name__ == "__main__":
    # Script entry point
    main()
```

### Database Connection
```python
from app.database import db

query = "SELECT * FROM table WHERE id = %s"
results = db.execute_query(query, (id_value,))
```

## DO NOT

- ❌ Modify frontend or backend code
- ❌ Use string formatting for SQL queries
- ❌ Skip type hints in Python scripts
- ❌ Ignore errors or use fallback values
- ❌ Create migrations without testing
- ❌ Hardcode configuration values
- ❌ Skip data validation
- ❌ Forget to handle NULL values in queries
- ❌ Create N+1 query problems
- ❌ Skip error handling

